 
 
SVM 
 
 
 
위의 식(b)이 0의 값을 가지는 선이 판단 기준이 된다. 
그리고 Y의 값을 -1과 1로 두어서, 저 식은 항상 양수의 값(d)을 갖는다. 
그러면 d 는 0점인 b와의 거리로 생각해도 되ㅁ나? 
예를 들어서 b는 엑스축으로 보고, d는 x축 과 수직선을 이을 수 있는 각각의 점이 되는 것이지. 이렇게 생각해도 맞을까? 
 
 
 
 
이 그래프를 색종이라고 생각하고, 저 굵은 선을 따라서 접는다고 생각하면, 
위의 말이 맞는 말일 수 있을 듯 하다.  
 
 
이제 드디어 수식이 이해가 제대로 되는 듯 하다. 
위의 문제는 Quadratic programmino 으로 푼다. 
 
 
 
 
자, 이제 소프트 마진 클래시파이어를 이해해 볼 차례. 
각 데이터 포인트에 대한 에러, 입실론 I 가 등장한다. 
-* 이거 잘 이해가 안된다. 이 부분 입실론. 
 
 
각 데이터 포인트의 오차 입실론을 합한 값은 페널티 패러미터 C보다 작아야 한다. 
 
 
페널티 패러미터란? 
오차를 얼마나 중시하는지 보여준다. Misclassification을 얼만큼 피하고 싶은지를 나타낸다. C가 큰 경우, 여유가 커진다. 반대의 경우, 여유가 작아진다. 여유가 작아지게 되면, misclassification이 될 확률이 높아지기 때문이다. 이렇게 생각하는게 맞을까? 
 
 

 
커널이 이런거구나. 데이터의 생김새를 좀 더 쉬운 모양으로 나타낼 수 있게, 새로운 공간으로 데이터를 옮겨버리는 함수. 그리고 새로운 공간 상 두 개 데이터의 유사한 정도를 측정하는 함수가 Kernal K(X) 
 
 
 
 
  함수 쎄타를 통해 2차원 상에 있던 점 라지엑스를 삼차원상의 점인 라지제트의 위치로 옮겼다. 2차원상에 있던 두 데이터 포인트를 각각 스몰 엑스 원, 스몰 엑스 투 라고 하자. 이 두 데이터 포인트 사이의 similarity가 어느 정도인지 커널 케이 함수를 통해 알아보자. 커널 케이 함수는 인풋 스몰 엑스 원과 스몰 엑스 투 사이의 이너 프로덕트, 내적의 값을 구하고, 그 값을 제곱한다. (왜?,,, 어짜피 양수일거ㅓㅓㅓㅓ가 아니구나.). 이 값을 삼차원 상의 데이터 포인트로 나타내면 위의 꺽은 괄호가 사용된 것과 같다.  
 
 
그래서, 커널 함수로 측정된 이 similarity가 이 모형에서 하는 역할이 무엇이지? 
 
 
그래서, svm/ kernel/ similarity/ role 을 키워드로 넣어 구글 검색을 해 보았다. 
커널에 대한 직관적인 설명 방법에 관한 의견을 구하는 (https://stats.stackexchange.com/questions/152897/how-to-intuitively-explain-what-a-kernel-is) 포스팅을 참고한 결과, 
 
 
커널은 두 개의 인풋을 받고, 그 둘이 서로 얼만큼 유사한지를 나타내는 similarity score를 뱉어내는 함수이다. 이 두 개의 인풋은 트리가 될 수도 벡터가 될수도, 정수가 될 수 도 있다. 그냥 커널이 유사도를 계산할 수 있는 두 개의 인풋이기만 하면 된다.  
 
 
가장 쉬운 커널의 예는 ‘내적’, 소위 말하는 ‘닷-프로덕트’ 이다. 두 개의 벡터가 주어졌을 때, 둘 사이의 유사도는 한 벡테를 다른 벡터에 대해 프로젝션시킨 벡터의 길이가 될 것이기 때문이다. (프로젝션:  두 벡터 a, b가 있을 때, 하나의 벡터 a 의 끝 점에서 다른 벡터 b 에 대해 수선을 내린다. 이 때  b위에 올라 앉은 a’이 프로젝션 된 벡터가 된다. 즉 시작점은 같았으나 향하는 방향은 서로 달랐던 두 벡터 a, b의 상태에서, 프로젝션을 행하고 나면, a’ 이 b 벡터가 가능 동일한 방향으로 진행하고 있다는 것이다. 아마 벡터 b 위에 a’이 누워 있는 꼴이 될 것이다. (가정: 벡터 a의 사이즈가 b 의 그것보다 작다.)) 
 
 
다른 재미있는 커널 함수의 예로 반지름과 관계 깊은 가우시안 커널을 이야기한다. 반지름의 크기 변화가 두 인풋의 유사도 측정에 역할을 하나보다. 그런데 난 이 가우시안 커널 식 모르니 지나가겠다. 
 
 
오 좋은 답변이 있다. 
 
 
 
 
그러니까 커널 덕택에 데이터를 직관적으로 이해하기 쉽게 만들기 위해 거쳐가야 하는 다차원의 세계를 안 갈 수 있게 되었다는 거잖아? 기존 데이터를 더 높은 차원에 매핑을 하고 거기에서 내적을 구하고 이렇게 두 단계 거칠 일을, 커널 함수 쓰면 한 단계만에 내적과 같은 값을 지닌 아웃풋을 내놓으니 편리하다, 이거지? 
 
 
또 다른 좋은 대답이 있다. 이거 좀 최고되는데, 
 
 
마지막 한 줄이 임팩트 있다. 그 그리스 로마 신화에서 지나가는 사람 붙잡아다가 자기가 가진 침대보다 크면 침대 넘어가는 팔 다리 자르고, 그보다 작으면 늘려서 죽인 그 자, Procrustes를 이야기한다. 커널이 이와 같은 역할을 한다는 것이다. 커널 함수가 서로 다른 데이터 사이의 유사한 정도를 체크하는 이유는 각 데이터에 대한 가중치를 부여하는 과정과 같다는 것이다. 예를 들어보자면, 커널 함수가 내놓는 값이 100이다. (스케일을 1-100으로 임으로 잡음. 실제로 그런지는 모름.) 두 데이터(a와 b)는 완전히 유사해. 그러면 두 개 카운트 할 필요가 없는거다. 한 데이터만 가지고서 클래시피케이션을 시행해도 문제가 전혀 없지. 그리고 이제 또 다른 데이터를 잡아온다 데이터 a와 c를 비교해 보니, 이 두 인풋이 내놓는 커널 함수 값은 20이야. 그러면 c는 죽일 필요가 없어. 살려두는거지. 두 데이터는 확실히 다르기 때문에 살려둘 필요가 있어. 이게 맞는 설명인지는 아직 확실치 않지만, 또 다른 내 이해에 따른 설명을 덧붙여 보겠다. 비선형 데이터를 선형 데이터로 바꿔 나가기 위해서, 중요 데이터 포인트만 추리는 과정이라고 볼 수는 없을까? 기준이 되는 한 포인트를 잡고(이게 서포트 벡터가 되려나?) 그 데이터 포인트와 상당히 이질적인 애들만 추려서 그래프를 다시 그려보는것이다. 유사도 점수에 기반해서. 그래서 높은 유사도를 가진 데이터는 쳐 내고, 낮은 유사도 가진 애들만 추려서 선형적인 형태로 만들어 보는 것이지.  
 
 
 
 
 
 
그리고 나는 페널티 패러미터 C가 여전히 헷갈렸다. 그래서 또 구글에 들어갔고, 만족스러운 대답을 찾을 수 있었다. (https://www.researchgate.net/post/In_support_vector_machinesSVM_how_we_adjust_the_parameter_C_why_we_use_this_parameter) 
 
 
 
 
 
 
여기서 파란 선은 페널티 패러미터 C를 작게 준 나머지, classifier로서의 general usage를 포기한 예에 해당한다. (오버피팅!) 
 
 
 
 
이 대답은 매우 이해하기 좋은 설명이라고 생각한다. 페널티 패러미터는 classifier의 편평한 정도와 서로 trade-off 관계를 가진다는 것이다. 페널티 패러미터가 커질수록 트레이닝 데이터 안에서의 오차는 작아질 것이다. 그러나 페널티 패러미터의 값을 지나치게 크게 잡게 되면 여기에서 구해지는 classifier는 generalized 될 당위를 잃게된다.  
 
 
반면, 만일 페널티 패러미터가 작은 값이면, 하이퍼플레인은 편평한 직선에 가까울 것이다. 따라서 여기서 C 값을 제대로 주어야 할 필요성이 발생한다. 트레이닝 데이터로 학습할 때의 에러 값을 작게 만드는 동시에, 테스트 데이터에서도 썩 괜찮은 성능을 보일 만큼의 C값이 이상적일 것이기 때문이다.  
 
 
 
 
 
 
 
 
 